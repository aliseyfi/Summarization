{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "summarization_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPYC3WkMquyi2eEXk0kICHq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayarghoshroy/Summarization/blob/master/summarization_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe_SA3n0I3T8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.neural_network import MLPRegressor as mlp\n",
        "from IPython.display import display\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTagdmneHA5f",
        "colab_type": "code",
        "outputId": "680346c0-d790-48f5-f24c-8e675d26ce4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "!pip install rouge\n",
        "from rouge import Rouge"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb1g4I5ARGTp",
        "colab_type": "code",
        "outputId": "193e13df-565b-4b70-bbe6-15f5fd776022",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UKpe_KPRLrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change to path to dataset\n",
        "file_name = \"/content/drive/My Drive/Summarization_Pickled_Data/cnn_dataset_1000_labelled.pkl\"\n",
        "stories = pickle.load(open(file_name, 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpKVZKCERXbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# displaying the first datapoint\n",
        "# verify correctness of load\n",
        "\n",
        "# Uncomment to Display the First Datapoint\n",
        "# print(stories[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsEbuI08RZ9J",
        "colab_type": "code",
        "outputId": "9a67edd2-6283-40e9-a289-a784f14d9a30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Required Models for glove\n",
        "# in case of errors with conda, use this:\n",
        "# conda install -c conda-forge spacy\n",
        "# this is what worked for me :P\n",
        "\n",
        "# !python -m spacy download en\n",
        "# !python -m spacy download en_core_web_lg\n",
        "!python -m spacy link en_core_web_lg en --force\n",
        "\n",
        "# use the large model as the default model for English textual data"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_lg -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-_iuFDvZTvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initializing the processor\n",
        "embedder = spacy.load('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEsDrJHUTOxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# basic embeddings using averaged glove vectors\n",
        "# using Spacy's large language model\n",
        "def get_embedding(text):\n",
        "    extract = embedder(text)\n",
        "    total_sum = np.zeros(300)\n",
        "    count = 0\n",
        "    for token in extract:\n",
        "        count += 1\n",
        "        total_sum += np.asarray(token.vector)\n",
        "    return total_sum / count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26X4T5jHRLpw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "44af2b50-587a-4f15-f3b8-0ad5fb0960ba"
      },
      "source": [
        "# creating the inputs and expected outputs\n",
        "train_size = 900\n",
        "val_size = 50\n",
        "test_size = 50\n",
        "\n",
        "def make_set(start_index, size):\n",
        "    count = 0\n",
        "    X_set = []\n",
        "    y_set = []\n",
        "\n",
        "    for count in tqdm(range(size)):\n",
        "        data = stories[start_index + count]\n",
        "\n",
        "        doc_emb = get_embedding(data['story_text'])\n",
        "        # use the function of choice to generate the document embedding\n",
        "\n",
        "        index = 0\n",
        "        for sentence in data['story']:\n",
        "            sent_emb = get_embedding(sentence)\n",
        "            # use the function of choice to generate the sentence embedding\n",
        "\n",
        "            x = np.concatenate((sent_emb, doc_emb))\n",
        "            try:\n",
        "                y = data['scores'][index]\n",
        "            except:\n",
        "                y = 0.0\n",
        "            index += 1\n",
        "\n",
        "            X_set.append(x)\n",
        "            y_set.append(y)\n",
        "\n",
        "    return np.asmatrix(X_set), np.asarray(y_set)\n",
        "\n",
        "X_train, y_train = make_set(0, train_size)\n",
        "X_val, y_val = make_set(train_size, val_size)\n",
        "X_test, y_test = make_set(train_size + val_size, test_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 900/900 [06:19<00:00,  2.37it/s]\n",
            "100%|██████████| 50/50 [00:19<00:00,  2.59it/s]\n",
            "100%|██████████| 50/50 [00:20<00:00,  2.49it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY6JkXtWWOwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_values(X, model):\n",
        "    return model.predict(X)\n",
        "\n",
        "def get_loss(pred, y):\n",
        "    return np.linalg.norm(pred - y) / np.shape(y)[0]\n",
        "\n",
        "model_name = \"glove_averaged\"\n",
        "# modify the model name\n",
        "\n",
        "def make_parameters(train_size):\n",
        "    batch_size = int(4 * np.sqrt(train_size))\n",
        "    n_batches = int(32 * (train_size / batch_size))\n",
        "    # can set batch_size to standard values such as 64, 128, 256\n",
        "\n",
        "    print(\"Total Number of Training Examples: \" + str(train_size))\n",
        "    print(\"Batch Size: \" + str(batch_size))\n",
        "    print(\"Number of Batches: \" + str(n_batches))\n",
        "\n",
        "    return batch_size, n_batches\n",
        "\n",
        "def train(X_train, y_train, batch_size, n_batches):\n",
        "    model = mlp(hidden_layer_sizes = (1024, 2048, 1024, 512, 256, 256, 128, 64), max_iter = 10000)\n",
        "    \n",
        "    train_size = np.shape(X_train)[0]\n",
        "\n",
        "    min_loss = 1e20\n",
        "\n",
        "    for iterator in tqdm(range(n_batches)):\n",
        "        idx = np.random.randint(0, train_size, size = batch_size)\n",
        "\n",
        "        X_select = X_train[idx,:]\n",
        "        y_select = y_train[idx]\n",
        "\n",
        "        model.partial_fit(X_select, y_select)\n",
        "\n",
        "        sentence_predicted_scores = get_values(X_val, model)\n",
        "\n",
        "        loss = get_loss(sentence_predicted_scores, y_val)\n",
        "\n",
        "        # saving best model seen so far\n",
        "        if loss < min_loss:\n",
        "            min_loss = loss\n",
        "            pickle.dump(model, open(model_name + '_best_model', 'wb'))\n",
        "\n",
        "    final_model = pickle.load(open(model_name + '_best_model', 'rb'))\n",
        "    return final_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrX21q4u8ukA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "9f49bb17-3e30-46f6-a0dd-beb4de345f1f"
      },
      "source": [
        "batch_size, n_batches = make_parameters(train_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Number of Training Examples: 900\n",
            "Batch Size: 120\n",
            "Number of Batches: 240\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZzWH2VWaQjZ",
        "colab_type": "code",
        "outputId": "b0a68bf1-db74-4582-8c25-b78f837c4e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "m = train(X_train, 1000 * y_train, batch_size, n_batches)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 240/240 [02:46<00:00,  1.44it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl4yFkiyiTa_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameter for similarity threshold\n",
        "theta = 0.95\n",
        "\n",
        "def similarity(A, B):\n",
        "    similarity =  (A @ B.T) / (np.linalg.norm(A) * np.linalg.norm(B))\n",
        "    return similarity\n",
        "\n",
        "def get_top_k(X_doc, y, k):\n",
        "    # k should be in {3, 4, 5}\n",
        "    # error handling\n",
        "    k = int(k)\n",
        "    if k > 5:\n",
        "        k = 5\n",
        "    elif k < 3:\n",
        "        k = 3\n",
        "    \n",
        "    order = np.flip(np.argsort(y))\n",
        "    sentence_set = []\n",
        "    for sent_id in order:\n",
        "        if sentence_set == []:\n",
        "            sentence_set.append(order[0])\n",
        "            continue\n",
        "\n",
        "        consider = X_doc[sent_id, :]\n",
        "        flag = 1\n",
        "        for consider_id in sentence_set:\n",
        "            if similarity(X_doc[consider_id, :], consider) > theta:\n",
        "                flag = 0\n",
        "                break\n",
        "\n",
        "        if flag == 1:\n",
        "            sentence_set.append(sent_id)\n",
        "    return sentence_set[0: min(k, len(sentence_set))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_zjPWDVG179",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating object of the ROUGE class\n",
        "rouge = Rouge()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph6OFgGwcewS",
        "colab_type": "code",
        "outputId": "bcb27a9a-a061-4a79-dbf2-e6d0cb97c0f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "# evaluation\n",
        "# testing out each document iteratively\n",
        "# test set: document 'train_size + val_size' onwards\n",
        "\n",
        "def join(lst):\n",
        "    string = \"\"\n",
        "    for elem in lst:\n",
        "        string = string + elem + \" . \"\n",
        "    return string\n",
        "\n",
        "def extract_rouge(rouge_dict):\n",
        "    scores = []\n",
        "\n",
        "    scores.append(100 * rouge_dict[\"rouge-1\"]['f'])\n",
        "    scores.append(100 * rouge_dict[\"rouge-1\"]['p'])\n",
        "    scores.append(100 * rouge_dict[\"rouge-1\"]['r'])\n",
        "\n",
        "    scores.append(100 * rouge_dict[\"rouge-2\"]['f'])\n",
        "    scores.append(100 * rouge_dict[\"rouge-2\"]['p'])\n",
        "    scores.append(100 * rouge_dict[\"rouge-2\"]['r'])\n",
        "\n",
        "    scores.append(100 * rouge_dict[\"rouge-l\"]['f'])\n",
        "    scores.append(100 * rouge_dict[\"rouge-l\"]['p'])\n",
        "    scores.append(100 * rouge_dict[\"rouge-l\"]['r'])\n",
        "\n",
        "    return np.asarray(scores)\n",
        "\n",
        "start_doc_id = train_size + val_size\n",
        "doc_count = len(stories)\n",
        "\n",
        "generated_summary, gold_summary = 0, 0\n",
        "# to access the final created summary\n",
        "\n",
        "# set the number of documents for testing\n",
        "limit = test_size\n",
        "\n",
        "result = {}\n",
        "result['3'] = np.zeros(9)\n",
        "result['4'] = np.zeros(9)\n",
        "result['5'] = np.zeros(9)\n",
        "# averaging the ROUGE Metrics\n",
        "# for different summary lengths\n",
        "\n",
        "count = 0\n",
        "\n",
        "while count < min(doc_count, limit):\n",
        "    X_doc = []\n",
        "    y_doc = []\n",
        "    data = stories[start_doc_id + count]\n",
        "    doc_emb = get_embedding(data['story_text'])\n",
        "\n",
        "    index = 0\n",
        "    for sentence in data['story']:\n",
        "        sent_emb = get_embedding(sentence)\n",
        "\n",
        "        x = np.concatenate((sent_emb, doc_emb))\n",
        "        try:\n",
        "            y = data['scores'][index]\n",
        "        except:\n",
        "            y = 0.0\n",
        "\n",
        "        index += 1\n",
        "\n",
        "        X_doc.append(x)\n",
        "        y_doc.append(y)\n",
        "\n",
        "    X_doc = np.asmatrix(X_doc)\n",
        "    y_doc = np.asarray(y_doc)\n",
        "\n",
        "    sentence_predicted_scores = get_values(X_doc, m)\n",
        "\n",
        "    loss = np.linalg.norm(sentence_predicted_scores - y_doc)\n",
        "\n",
        "    # Uncomment to view the test_loss on the sample  \n",
        "    # print(loss)\n",
        "\n",
        "    gold_summary = join(data['highlights'])\n",
        "\n",
        "    for k in [3, 4, 5]:\n",
        "        summary_sent_id = get_top_k(X_doc, sentence_predicted_scores, k)\n",
        "        # Uncomment to view the indices of chosen sentences\n",
        "        # print(\"Document ID:\", start_doc_id + count, \", Top 5 Sentences:\", summary_sent_id)\n",
        "\n",
        "        # Uncomment to view the top 10 sentences based on Gold Labels\n",
        "        # print(\"Top 10 sentences based on Gold Label\", np.ndarray.tolist(np.flip(np.argsort(y_doc))[0:10]))\n",
        "\n",
        "        generated_summary = join([data['story'][idx] for idx in summary_sent_id])\n",
        "\n",
        "        scores = rouge.get_scores(generated_summary, gold_summary)[0]\n",
        "        result[str(k)] += extract_rouge(scores)\n",
        "\n",
        "    count += 1\n",
        "\n",
        "for k in [3, 4, 5]:\n",
        "    result[str(k)] = result[str(k)] / test_size\n",
        "\n",
        "predicted = get_values(X_test, m)\n",
        "test_loss = get_loss(y_test, predicted)\n",
        "\n",
        "print(\"Sample Output:\")\n",
        "print(\"Document:\\n\", stories[-1]['story_text'])\n",
        "print(\"Generated Summary:\\n\", generated_summary)\n",
        "print(\"Gold Summary:\\n\", gold_summary)\n",
        "\n",
        "print(\"\\nAll Metrics:\\n\")\n",
        "\n",
        "data = []\n",
        "for k in [3, 4, 5]:\n",
        "    lst = np.ndarray.tolist(result[str(k)])\n",
        "    lst.append(test_loss)\n",
        "    data.append(lst)\n",
        "\n",
        "df = pd.DataFrame(data, columns = ['R1-f', 'R1-p', 'R1-r',\n",
        "                                    'R2-f', 'R2-p', 'R2-r',\n",
        "                                    'Rl-f', 'Rl-p', 'Rl-r',\n",
        "                                    'Loss'], dtype = float)\n",
        "\n",
        "df.index = ['glove top-3', 'glove top-4', 'glove top-5']\n",
        "display(df)\n",
        "\n",
        "# save results into a dataframe file\n",
        "df.to_csv(model_name + '_results.csv')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample Output:\n",
            "Document:\n",
            " -- in the wake of the earthquake in haiti, george clooney and other celebrities have signed on for a telethon to aid the devastated island nation..the oscar-winning actor will take part in a fundraising program to air commercial-free across several networks, mtv announced..\"hope for haiti now\" will air on abc, cbs, nbc, fox, cnn, bet, the cw, hbo, mtv, vh1 and cmt starting at 8 p.m. et/pt and 7 p.m. ct on friday, january 22..mtv said clooney will serve as a host in los angeles, while musician wyclef jean will be in new york, and cnn's anderson cooper will appear from haiti..the two-hour event will feature as-yet-unnamed musical performances and celebrity appearances, as well as live news reports from cnn..it's hollywood's latest philanthropic gesture in reaction to the catastrophic situation in haiti..a celebrity lounge at this weekend's golden globe awards in beverly hills has been turned into a haitian aid fundraiser..medecins sans frontieres says actors angelina jolie and brad pitt have donated $1 million to the group's emergency medical operation as it responds to the disaster..full coverage of the earthquake in haiti.tuesday's 7.0 earthquake has devastated the poverty-stricken country's infrastructure. haitian president rene preval said wednesday that he had heard estimates of up to 50,000 dead but that it was too early to know for sure..damage has closed the port and limited airport operations in the capital city of port-au-prince, and the quake buckled many roads, making it extremely difficult for aid groups to bring in emergency supplies and search for survivors in the rubble..mtv said all proceeds from the telethon will be split evenly among seven relief organizations currently operating in haiti: clinton-bush haiti fund, oxfam america, partners in health, the red cross, unicef and yele haiti foundation and world food programme..\n",
            "Generated Summary:\n",
            " full coverage of the earthquake in haiti . damage has closed the port and limited airport operations in the capital city of port-au-prince, and the quake buckled many roads, making it extremely difficult for aid groups to bring in emergency supplies and search for survivors in the rubble. . the oscar-winning actor will take part in a fundraising program to air commercial-free across several networks, mtv announced. . tuesday's 7.0 earthquake has devastated the poverty-stricken country's infrastructure. haitian president rene preval said wednesday that he had heard estimates of up to 50,000 dead but that it was too early to know for sure. . it's hollywood's latest philanthropic gesture in reaction to the catastrophic situation in haiti. . \n",
            "Gold Summary:\n",
            " fundraising program to be telecast on numerous networks on friday, january 22 . all proceeds will be split among five relief organizations . other celebrities have already launched efforts to aid quake-ravaged haiti . \n",
            "\n",
            "All Metrics:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>R1-f</th>\n",
              "      <th>R1-p</th>\n",
              "      <th>R1-r</th>\n",
              "      <th>R2-f</th>\n",
              "      <th>R2-p</th>\n",
              "      <th>R2-r</th>\n",
              "      <th>Rl-f</th>\n",
              "      <th>Rl-p</th>\n",
              "      <th>Rl-r</th>\n",
              "      <th>Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>glove top-3</th>\n",
              "      <td>19.863578</td>\n",
              "      <td>17.084033</td>\n",
              "      <td>27.306880</td>\n",
              "      <td>3.656077</td>\n",
              "      <td>3.076394</td>\n",
              "      <td>5.120395</td>\n",
              "      <td>19.626265</td>\n",
              "      <td>17.409745</td>\n",
              "      <td>25.007215</td>\n",
              "      <td>0.021319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>glove top-4</th>\n",
              "      <td>21.221096</td>\n",
              "      <td>16.381640</td>\n",
              "      <td>33.794695</td>\n",
              "      <td>4.851359</td>\n",
              "      <td>3.628973</td>\n",
              "      <td>7.988641</td>\n",
              "      <td>21.416701</td>\n",
              "      <td>17.196760</td>\n",
              "      <td>30.873652</td>\n",
              "      <td>0.021319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>glove top-5</th>\n",
              "      <td>20.495349</td>\n",
              "      <td>14.758630</td>\n",
              "      <td>37.482070</td>\n",
              "      <td>4.694902</td>\n",
              "      <td>3.298677</td>\n",
              "      <td>8.925741</td>\n",
              "      <td>20.879048</td>\n",
              "      <td>15.715715</td>\n",
              "      <td>33.842064</td>\n",
              "      <td>0.021319</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  R1-f       R1-p       R1-r  ...       Rl-p       Rl-r      Loss\n",
              "glove top-3  19.863578  17.084033  27.306880  ...  17.409745  25.007215  0.021319\n",
              "glove top-4  21.221096  16.381640  33.794695  ...  17.196760  30.873652  0.021319\n",
              "glove top-5  20.495349  14.758630  37.482070  ...  15.715715  33.842064  0.021319\n",
              "\n",
              "[3 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uZR5MRhhD9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ^_^ Thank You"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}